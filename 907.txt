Why does dividing a number by a decimal create such a large number ? I ca n't for the life of me grasp how this works :( Explain like I'm five.<>Smaller numbers go in to bigger number more times . So the answers are bigger .<>Dividing a number by a decimal is similar to dividing a number by a fraction. When you divide a number by a fraction, you are asking \"how many times does this number fit into the other number?\", For example, if you want to know how many times 4 fits into 20, you can divide 20 by 4 to get the answer: 20 \/ 4 = 5. This means that 4 fits into 20 five times. A decimal is just a special kind of fraction, where the fraction has a denominator (the number on the bottom) that is a power of 10. For example, 0.1 is equal to 1\/10, and 0.01 is equal to 1\/100. If you divide a number by a decimal, it's like you are dividing the number by a very small fraction. For example, if you divide 20 by 0.1, you are asking \"how many times does 0.1 fit into 20?\" The answer is 200, because 0.1 fits into 20 200 times. So when you divide a number by a decimal, the result is usually a larger number because the decimal is like a very small fraction, and the result of dividing by a small fraction is usually a large number. I hope this helps! Let me know if you have any more questions.<>reddit_eli5